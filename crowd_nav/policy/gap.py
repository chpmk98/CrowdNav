import torch
import torch.nn as nn
from torch.nn.functional import softmax
import logging
from crowd_nav.policy.cadrl import mlp
from crowd_nav.policy.multi_human_rl import MultiHumanRL


class ValueNetwork(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.input_dim = input_dim # input_dim is the variables in robot state + one human state
        self.mlp1 = mlp(input_dim, [150, 100], last_relu=True)
        self.mlp2 = mlp(100, [100, 50]) # input is self.mlp1
        self.mlp3 = mlp(200, [100, 100, 1]) # input is cat(self.mlp1, mean(self.mlp1))
        self.mlp4 = mlp(50, [150, 100, 100], last_relu=True) # input is sum(self.mlp3 * self.mlp2)
        self.lin1 = mlp(100, [81]) # input is self.mlp4
        self.lin2 = mlp(100, [1]) # input is self.mlp4
        
        # used when reformatting the mean of mlp1 output
        self.global_state_dim = 100

    def forward(self, state):
        """
        First transform the world coordinates to self-centric coordinates and then do forward computation

        :param state: tensor of shape (batch_size, # of humans, length of a rotated state)
                      generated by MultiHumanRL.transform(state)
        :return:
        """
        size = state.shape # (batch_size, # humans, self.input_dim)
        # reshape input to be used in the network
        self_state = state[:, 0, :self.self_state_dim]
        mlp1_output = self.mlp1(state.view((-1, size[2]))) # (batch_size * # humans, self.mlp1)
        mlp2_output = self.mlp2(mlp1_output) # (batch_size * # humans, self.mlp2)
        
        # compute mean across humans in each batch
        global_state = torch.mean(mlp1_output.view(size[0], size[1], -1), 1, keepdim=True) # (batch_size, 1, self.mlp1)
        global_state = global_state.expand((size[0], size[1], self.global_state_dim)).\
                contiguous().view(-1, self.global_state_dim) # (batch_size * # humans, self.mlp1)
        # calculate alphas
        mlp3_input = torch.cat([mlp1_output, global_state], dim=1) # (batch_size * # humans, 2 * self.mlp1)
        mlp3_output = self.mlp3(mlp3_input).view(size[0], size[1], 1).squeeze(dim=2) # (batch_size, # humans)
        alpha = softmax(mlp3_output, dim=1).unsqueeze(2) # (batch_size, # humans, 1)
        
        # calculate sum of mlp2 weighted by alpha and feed it through mlp4
        mlp4_input = torch.sum(torch.mul(alpha, mlp2_output.view(size[0], size[1], -1)), dim=1) # (batch_size, self.mlp2)
        mlp4_output = self.mlp4(mlp4_input) # (batch_size, self.mlp4)
        
        # calculate policy and value and return
        pi = softmax(self.lin1(mlp4_output), dim=1) # (batch_size, self.lin1)
        val = self.lin2(mlp4_output) # (batch_size, self.lin2)
        
        return pi, val


class GAP(MultiHumanRL):
    def __init__(self):
        super().__init__()
        self.name = 'GAP'

    def configure(self, config):
        self.input_dim = config.get('gap', 'input_dim')
        self.model = ValueNetwork(self.input_dim) # used to be MultiHumanRL.input_dim()
        logging.info('Policy: {}'.format(self.name))
